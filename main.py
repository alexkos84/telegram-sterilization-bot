import os
import requests
import time
import random
from bs4 import BeautifulSoup
import re
from datetime import datetime, timedelta
from typing import Dict, List, Optional
import json
import logging
from urllib.parse import quote_plus, unquote
import base64

logger = logging.getLogger(__name__)

class SuperRobustTelegramParser:
    """–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ —É—Å—Ç–æ–π—á–∏–≤—ã–π –ø–∞—Ä—Å–µ—Ä —Å –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–º–∏ —Å—Ç—Ä–∞—Ç–µ–≥–∏—è–º–∏"""
    
    def __init__(self):
        self.groups = [
            {
                'username': 'lapki_ruchki_yalta',
                'url': 'https://t.me/lapki_ruchki_yalta',
                'type': 'cats'
            },
            {
                'username': 'yalta_aninmals',
                'url': 'https://t.me/yalta_aninmals',
                'type': 'dogs'
            }
        ]
        
        self.posts_cache = []
        self.backup_cache = []  # –†–µ–∑–µ—Ä–≤–Ω—ã–π –∫—ç—à –¥–ª—è –¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω–æ–≥–æ —Ö—Ä–∞–Ω–µ–Ω–∏—è
        self.last_update = None
        self.last_attempt = None
        self.failure_count = 0
        self.success_count = 0
        
        # –ú–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ User-Agents (–±–æ–ª—å—à–µ –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤)
        self.user_agents = [
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.0 Safari/605.1.15',
            'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:121.0) Gecko/20100101 Firefox/121.0',
            'Mozilla/5.0 (iPhone; CPU iPhone OS 17_0 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.0 Mobile/15E148 Safari/604.1',
            'Mozilla/5.0 (Android 13; Mobile; rv:109.0) Gecko/109.0 Firefox/109.0',
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/118.0.0.0 YaBrowser/23.11.0.0 Safari/537.36',
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36 Edg/119.0.0.0'
        ]
        
        # –ü—Ä–æ–∫—Å–∏ —Å–µ—Ä–≤–∏—Å—ã (–±–µ—Å–ø–ª–∞—Ç–Ω—ã–µ)
        self.proxy_sources = [
            'https://free-proxy-list.net/',
            'https://www.proxy-list.download/api/v1/get?type=http',
            'https://raw.githubusercontent.com/clarketm/proxy-list/master/proxy-list-raw.txt'
        ]
        
        self.working_proxies = []
        self.proxy_check_time = None
        
        # RSS –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤—ã (–µ—Å–ª–∏ –µ—Å—Ç—å)
        self.rss_alternatives = [
            # –ù–µ–∫–æ—Ç–æ—Ä—ã–µ Telegram –∫–∞–Ω–∞–ª—ã –∏–º–µ—é—Ç RSS
            f'https://rsshub.app/telegram/channel/lapki_ruchki_yalta',
            f'https://rsshub.app/telegram/channel/yalta_aninmals'
        ]
        
        # –ó–µ—Ä–∫–∞–ª–∞ –∏ –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–µ –¥–æ–º–µ–Ω—ã
        self.telegram_mirrors = [
            'https://t.me/s/',
            'https://telegram.me/s/',
            'https://web.telegram.org/k/',
            'https://telegram.dog/s/',
            'https://tg.i-c-a.su/s/'  # –†–æ—Å—Å–∏–π—Å–∫–æ–µ –∑–µ—Ä–∫–∞–ª–æ
        ]
        
        # –ü–æ–∏—Å–∫ —á–µ—Ä–µ–∑ –ø–æ–∏—Å–∫–æ–≤–∏–∫–∏
        self.search_engines = [
            'https://www.google.com/search?q=site:t.me/',
            'https://yandex.ru/search/?text=site:t.me/',
            'https://duckduckgo.com/?q=site:t.me+'
        ]

    def should_attempt_parsing(self) -> bool:
        """–£–º–Ω–∞—è –ª–æ–≥–∏–∫–∞ –ø–æ–ø—ã—Ç–æ–∫ –ø–∞—Ä—Å–∏–Ω–≥–∞"""
        if not self.last_attempt:
            return True
        
        # –≠–∫—Å–ø–æ–Ω–µ–Ω—Ü–∏–∞–ª—å–Ω–æ–µ —É–≤–µ–ª–∏—á–µ–Ω–∏–µ –∑–∞–¥–µ—Ä–∂–∫–∏, –Ω–æ —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ–º
        if self.success_count > 0:
            # –ï—Å–ª–∏ –±—ã–ª–∏ —É—Å–ø–µ—Ö–∏, –¥–µ–ª–∞–µ–º –ø–æ–ø—ã—Ç–∫–∏ —á–∞—â–µ
            max_cooldown = min(self.failure_count * 2, 15)  # –º–∞–∫—Å–∏–º—É–º 15 –º–∏–Ω—É—Ç
        else:
            # –ï—Å–ª–∏ —É—Å–ø–µ—Ö–æ–≤ –Ω–µ –±—ã–ª–æ, —É–≤–µ–ª–∏—á–∏–≤–∞–µ–º –∏–Ω—Ç–µ—Ä–≤–∞–ª
            max_cooldown = min(self.failure_count * 5, 60)  # –º–∞–∫—Å–∏–º—É–º —á–∞—Å
        
        time_passed = (datetime.now() - self.last_attempt).total_seconds() / 60
        return time_passed > max_cooldown

    def get_random_headers(self):
        """–°–ª—É—á–∞–π–Ω—ã–µ –∑–∞–≥–æ–ª–æ–≤–∫–∏ —Å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏"""
        headers = {
            'User-Agent': random.choice(self.user_agents),
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8',
            'Accept-Language': random.choice([
                'ru-RU,ru;q=0.9,en;q=0.8',
                'en-US,en;q=0.9,ru;q=0.8',
                'uk-UA,uk;q=0.9,ru;q=0.8,en;q=0.7'
            ]),
            'Accept-Encoding': 'gzip, deflate, br',
            'DNT': '1',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'Sec-Fetch-Dest': 'document',
            'Sec-Fetch-Mode': 'navigate',
            'Sec-Fetch-Site': 'none',
            'Cache-Control': 'max-age=0',
        }
        
        # –°–ª—É—á–∞–π–Ω—ã–µ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –∑–∞–≥–æ–ª–æ–≤–∫–∏
        if random.choice([True, False]):
            headers['Referer'] = random.choice([
                'https://www.google.com/',
                'https://yandex.ru/',
                'https://telegram.org/'
            ])
        
        if random.choice([True, False]):
            headers['X-Requested-With'] = 'XMLHttpRequest'
        
        return headers

    def get_working_proxies(self) -> List[Dict]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Ä–∞–±–æ—á–∏—Ö –ø—Ä–æ–∫—Å–∏"""
        if (self.working_proxies and self.proxy_check_time and 
            (datetime.now() - self.proxy_check_time).seconds < 3600):  # –ö—ç—à –Ω–∞ —á–∞—Å
            return self.working_proxies
        
        proxies = []
        
        try:
            # –ü—Ä–æ—Å—Ç–æ–π —Å–ø–∏—Å–æ–∫ –±–µ—Å–ø–ª–∞—Ç–Ω—ã—Ö –ø—Ä–æ–∫—Å–∏ (–∑–∞–º–µ–Ω–∏—Ç–µ –Ω–∞ –∞–∫—Ç—É–∞–ª—å–Ω—ã–µ)
            test_proxies = [
                {'http': 'http://185.162.230.55:80', 'https': 'http://185.162.230.55:80'},
                {'http': 'http://103.152.112.162:80', 'https': 'http://103.152.112.162:80'},
                # –î–æ–±–∞–≤—å—Ç–µ –∞–∫—Ç—É–∞–ª—å–Ω—ã–µ –ø—Ä–æ–∫—Å–∏ –∏–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –ø–∞—Ä—Å–∏–Ω–≥
            ]
            
            for proxy in test_proxies:
                try:
                    response = requests.get(
                        'http://httpbin.org/ip', 
                        proxies=proxy, 
                        timeout=5
                    )
                    if response.status_code == 200:
                        proxies.append(proxy)
                        if len(proxies) >= 3:  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ
                            break
                except:
                    continue
            
            self.working_proxies = proxies
            self.proxy_check_time = datetime.now()
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –ø—Ä–æ–∫—Å–∏: {e}")
        
        return proxies

    def get_group_posts(self, group_type: str = 'all', limit: int = 3) -> List[Dict]:
        """–ì–ª–∞–≤–Ω—ã–π –º–µ—Ç–æ–¥ —Å –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–º–∏ —Å—Ç—Ä–∞—Ç–µ–≥–∏—è–º–∏"""
        self.last_attempt = datetime.now()
        
        if not self.should_attempt_parsing():
            logger.info(f"‚è≥ –ü–∞—Ä—Å–∏–Ω–≥ –ø—Ä–æ–ø—É—â–µ–Ω (–∫—É–ª–¥–∞—É–Ω)")
            return self.get_fallback_posts(group_type, limit)
        
        strategies = [
            self.strategy_direct_parsing,      # –ü—Ä—è–º–æ–π –ø–∞—Ä—Å–∏–Ω–≥
            self.strategy_mirror_parsing,      # –ß–µ—Ä–µ–∑ –∑–µ—Ä–∫–∞–ª–∞
            self.strategy_proxy_parsing,       # –ß–µ—Ä–µ–∑ –ø—Ä–æ–∫—Å–∏
            self.strategy_rss_parsing,         # RSS —Ñ–∏–¥—ã
            self.strategy_search_parsing,      # –ß–µ—Ä–µ–∑ –ø–æ–∏—Å–∫
            self.strategy_cache_rotation,      # –†–æ—Ç–∞—Ü–∏—è –∫—ç—à–∞
        ]
        
        posts = []
        
        for i, strategy in enumerate(strategies):
            try:
                logger.info(f"üîÑ –°—Ç—Ä–∞—Ç–µ–≥–∏—è {i+1}: {strategy.__name__}")
                result = strategy(group_type, limit)
                
                if result and len(result) > 0:
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–∞—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
                    valid_posts = [p for p in result if self.validate_post(p)]
                    
                    if valid_posts:
                        posts = valid_posts
                        self.success_count += 1
                        self.failure_count = max(0, self.failure_count - 1)
                        logger.info(f"‚úÖ –°—Ç—Ä–∞—Ç–µ–≥–∏—è {i+1} —É—Å–ø–µ—à–Ω–∞: {len(posts)} –ø–æ—Å—Ç–æ–≤")
                        break
                
                # –ü–∞—É–∑–∞ –º–µ–∂–¥—É —Å—Ç—Ä–∞—Ç–µ–≥–∏—è–º–∏
                time.sleep(random.uniform(1, 3))
                
            except Exception as e:
                logger.error(f"‚ùå –°—Ç—Ä–∞—Ç–µ–≥–∏—è {i+1} –Ω–µ—É–¥–∞—á–Ω–∞: {e}")
                continue
        
        if posts:
            self.posts_cache = posts
            self.backup_cache = posts.copy()  # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ —Ä–µ–∑–µ—Ä–≤
            self.last_update = datetime.now()
            logger.info(f"‚úÖ –ü–æ–ª—É—á–µ–Ω–æ {len(posts)} –ø–æ—Å—Ç–æ–≤")
        else:
            self.failure_count += 1
            logger.warning(f"‚ùå –í—Å–µ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ –Ω–µ—É–¥–∞—á–Ω—ã (–ø–æ–ø—ã—Ç–∫–∞ #{self.failure_count})")
            posts = self.get_fallback_posts(group_type, limit)
        
        return posts

    def strategy_direct_parsing(self, group_type: str, limit: int) -> List[Dict]:
        """–°—Ç—Ä–∞—Ç–µ–≥–∏—è 1: –ü—Ä—è–º–æ–π –ø–∞—Ä—Å–∏–Ω–≥ —Å —É–ª—É—á—à–µ–Ω–∏—è–º–∏"""
        posts = []
        
        for group in self.groups:
            if group_type != 'all' and group['type'] != group_type:
                continue
            
            for attempt in range(3):
                try:
                    session = requests.Session()
                    session.headers.update(self.get_random_headers())
                    
                    # –î–æ–±–∞–≤–ª—è–µ–º —Å–ª—É—á–∞–π–Ω—É—é –∑–∞–¥–µ—Ä–∂–∫—É
                    time.sleep(random.uniform(1, 4))
                    
                    url = f'https://t.me/s/{group["username"]}'
                    logger.info(f"üåê –ü—Ä—è–º–æ–π –ø–∞—Ä—Å–∏–Ω–≥: {url} (–ø–æ–ø—ã—Ç–∫–∞ {attempt + 1})")
                    
                    response = session.get(
                        url, 
                        timeout=20,
                        allow_redirects=True,
                        verify=True
                    )
                    
                    if response.status_code == 200 and len(response.text) > 5000:
                        group_posts = self.parse_html_content(response.text, group, limit)
                        if group_posts:
                            posts.extend(group_posts)
                            logger.info(f"‚úÖ –ü–æ–ª—É—á–µ–Ω–æ {len(group_posts)} –ø–æ—Å—Ç–æ–≤")
                            break
                    
                    # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –∑–∞–¥–µ—Ä–∂–∫—É –ø—Ä–∏ –Ω–µ—É–¥–∞—á–µ
                    time.sleep(random.uniform(2, 6))
                    
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è –ü–æ–ø—ã—Ç–∫–∞ {attempt + 1}: {e}")
                    if attempt < 2:
                        time.sleep(random.uniform(3, 7))
        
        return posts

    def strategy_mirror_parsing(self, group_type: str, limit: int) -> List[Dict]:
        """–°—Ç—Ä–∞—Ç–µ–≥–∏—è 2: –ü–∞—Ä—Å–∏–Ω–≥ —á–µ—Ä–µ–∑ –∑–µ—Ä–∫–∞–ª–∞"""
        posts = []
        
        for group in self.groups:
            if group_type != 'all' and group['type'] != group_type:
                continue
            
            for mirror in self.telegram_mirrors:
                try:
                    session = requests.Session()
                    session.headers.update(self.get_random_headers())
                    
                    url = f"{mirror}{group['username']}"
                    logger.info(f"ü™û –ó–µ—Ä–∫–∞–ª–æ: {url}")
                    
                    response = session.get(url, timeout=15)
                    
                    if response.status_code == 200 and len(response.text) > 3000:
                        group_posts = self.parse_html_content(response.text, group, limit)
                        if group_posts:
                            posts.extend(group_posts)
                            logger.info(f"‚úÖ –ó–µ—Ä–∫–∞–ª–æ —Ä–∞–±–æ—Ç–∞–µ—Ç: {len(group_posts)} –ø–æ—Å—Ç–æ–≤")
                            break
                    
                    time.sleep(random.uniform(2, 4))
                    
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è –ó–µ—Ä–∫–∞–ª–æ {mirror}: {e}")
                    continue
        
        return posts

    def strategy_proxy_parsing(self, group_type: str, limit: int) -> List[Dict]:
        """–°—Ç—Ä–∞—Ç–µ–≥–∏—è 3: –ü–∞—Ä—Å–∏–Ω–≥ —á–µ—Ä–µ–∑ –ø—Ä–æ–∫—Å–∏"""
        posts = []
        proxies = self.get_working_proxies()
        
        if not proxies:
            logger.info("üö´ –†–∞–±–æ—á–∏—Ö –ø—Ä–æ–∫—Å–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ")
            return posts
        
        for group in self.groups:
            if group_type != 'all' and group['type'] != group_type:
                continue
            
            for proxy in proxies:
                try:
                    session = requests.Session()
                    session.headers.update(self.get_random_headers())
                    session.proxies = proxy
                    
                    url = f'https://t.me/s/{group["username"]}'
                    logger.info(f"üîÄ –ü—Ä–æ–∫—Å–∏ –ø–∞—Ä—Å–∏–Ω–≥: {url}")
                    
                    response = session.get(url, timeout=20)
                    
                    if response.status_code == 200:
                        group_posts = self.parse_html_content(response.text, group, limit)
                        if group_posts:
                            posts.extend(group_posts)
                            logger.info(f"‚úÖ –ü—Ä–æ–∫—Å–∏ —Ä–∞–±–æ—Ç–∞–µ—Ç: {len(group_posts)} –ø–æ—Å—Ç–æ–≤")
                            break
                    
                    time.sleep(random.uniform(2, 5))
                    
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è –ü—Ä–æ–∫—Å–∏ –æ—à–∏–±–∫–∞: {e}")
                    continue
        
        return posts

    def strategy_rss_parsing(self, group_type: str, limit: int) -> List[Dict]:
        """–°—Ç—Ä–∞—Ç–µ–≥–∏—è 4: RSS —Ñ–∏–¥—ã"""
        posts = []
        
        for group in self.groups:
            if group_type != 'all' and group['type'] != group_type:
                continue
            
            # RSS —á–µ—Ä–µ–∑ RSShub (–µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω)
            rss_urls = [
                f'https://rsshub.app/telegram/channel/{group["username"]}',
                f'https://rss.app/feeds/telegram/{group["username"]}.xml',
                # –î–æ–±–∞–≤—å—Ç–µ –¥—Ä—É–≥–∏–µ RSS —Å–µ—Ä–≤–∏—Å—ã
            ]
            
            for rss_url in rss_urls:
                try:
                    session = requests.Session()
                    session.headers.update(self.get_random_headers())
                    
                    logger.info(f"üì° RSS: {rss_url}")
                    response = session.get(rss_url, timeout=15)
                    
                    if response.status_code == 200 and 'xml' in response.headers.get('content-type', ''):
                        rss_posts = self.parse_rss_content(response.text, group, limit)
                        if rss_posts:
                            posts.extend(rss_posts)
                            logger.info(f"‚úÖ RSS —Ä–∞–±–æ—Ç–∞–µ—Ç: {len(rss_posts)} –ø–æ—Å—Ç–æ–≤")
                            break
                    
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è RSS –æ—à–∏–±–∫–∞: {e}")
                    continue
        
        return posts

    def strategy_search_parsing(self, group_type: str, limit: int) -> List[Dict]:
        """–°—Ç—Ä–∞—Ç–µ–≥–∏—è 5: –ü–æ–∏—Å–∫ —á–µ—Ä–µ–∑ –ø–æ–∏—Å–∫–æ–≤–∏–∫–∏"""
        posts = []
        
        for group in self.groups:
            if group_type != 'all' and group['type'] != group_type:
                continue
            
            # –ü–æ–∏—Å–∫ –ø–æ—Å–ª–µ–¥–Ω–∏—Ö –ø–æ—Å—Ç–æ–≤ —á–µ—Ä–µ–∑ Google/Yandex
            search_queries = [
                f'{group["username"]} –∫–æ—Ç –∫–æ—à–∫–∞ –¥–æ–º site:t.me',
                f'{group["username"]} —Å–æ–±–∞–∫–∞ —â–µ–Ω–æ–∫ –ø—Ä–∏—Å—Ç—Ä–æ–π—Å—Ç–≤–æ site:t.me',
                f'"{group["username"]}" –∂–∏–≤–æ—Ç–Ω—ã–µ –∏—â–µ—Ç –¥–æ–º'
            ]
            
            for query in search_queries:
                try:
                    # –ò—Å–ø–æ–ª—å–∑—É–µ–º DuckDuckGo (–º–µ–Ω–µ–µ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω)
                    search_url = f"https://duckduckgo.com/html/?q={quote_plus(query)}"
                    
                    session = requests.Session()
                    session.headers.update(self.get_random_headers())
                    
                    logger.info(f"üîç –ü–æ–∏—Å–∫: {query[:50]}...")
                    response = session.get(search_url, timeout=15)
                    
                    if response.status_code == 200:
                        search_posts = self.parse_search_results(response.text, group, limit)
                        if search_posts:
                            posts.extend(search_posts)
                            logger.info(f"‚úÖ –ü–æ–∏—Å–∫ —Ä–∞–±–æ—Ç–∞–µ—Ç: {len(search_posts)} –ø–æ—Å—Ç–æ–≤")
                            break
                    
                    time.sleep(random.uniform(3, 6))  # –ë–æ–ª—å—à–∞—è –ø–∞—É–∑–∞ –¥–ª—è –ø–æ–∏—Å–∫–æ–≤–∏–∫–æ–≤
                    
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è –ü–æ–∏—Å–∫ –æ—à–∏–±–∫–∞: {e}")
                    continue
        
        return posts

    def strategy_cache_rotation(self, group_type: str, limit: int) -> List[Dict]:
        """–°—Ç—Ä–∞—Ç–µ–≥–∏—è 6: –£–º–Ω–∞—è —Ä–æ—Ç–∞—Ü–∏—è –∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö"""
        posts = []
        
        if self.backup_cache:
            # –ë–µ—Ä–µ–º –∏–∑ —Ä–µ–∑–µ—Ä–≤–Ω–æ–≥–æ –∫—ç—à–∞ –∏ –Ω–µ–º–Ω–æ–≥–æ –º–æ–¥–∏—Ñ–∏—Ü–∏—Ä—É–µ–º
            cached_posts = [p for p in self.backup_cache 
                          if group_type == 'all' or p['type'] == group_type]
            
            if cached_posts:
                # –û–±–Ω–æ–≤–ª—è–µ–º –¥–∞—Ç—ã –∏ –Ω–µ–º–Ω–æ–≥–æ –∏–∑–º–µ–Ω—è–µ–º –¥–∞–Ω–Ω—ã–µ
                for post in cached_posts[:limit]:
                    updated_post = post.copy()
                    
                    # –û–±–Ω–æ–≤–ª—è–µ–º –¥–∞—Ç—É –Ω–∞ –±–æ–ª–µ–µ —Å–≤–µ–∂—É—é
                    old_date = datetime.now() - timedelta(
                        days=random.randint(0, 3),
                        hours=random.randint(0, 23)
                    )
                    updated_post['date'] = old_date.strftime('%d.%m.%Y %H:%M')
                    
                    # –ü–æ–º–µ—á–∞–µ–º –∫–∞–∫ –∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω—ã–π
                    updated_post['source'] = 'cached_rotation'
                    posts.append(updated_post)
                
                logger.info(f"‚ôªÔ∏è –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω —Ä–æ—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∫—ç—à: {len(posts)} –ø–æ—Å—Ç–æ–≤")
        
        return posts

    def parse_rss_content(self, xml_content: str, group: Dict, limit: int) -> List[Dict]:
        """–ü–∞—Ä—Å–∏–Ω–≥ RSS –∫–æ–Ω—Ç–µ–Ω—Ç–∞"""
        try:
            from xml.etree import ElementTree as ET
            root = ET.fromstring(xml_content)
            
            posts = []
            items = root.findall('.//item')[:limit*2]  # –ë–µ—Ä–µ–º –±–æ–ª—å—à–µ –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏
            
            for item in items:
                try:
                    title = item.find('title').text if item.find('title') is not None else ''
                    description = item.find('description').text if item.find('description') is not None else ''
                    link = item.find('link').text if item.find('link') is not None else ''
                    pub_date = item.find('pubDate').text if item.find('pubDate') is not None else ''
                    
                    # –ò–∑–≤–ª–µ–∫–∞–µ–º ID –∏–∑ —Å—Å—ã–ª–∫–∏
                    post_id = link.split('/')[-1] if link else f"rss_{hash(title) % 10000}"
                    
                    # –§–æ—Ä–º–∏—Ä—É–µ–º –ø–æ—Å—Ç
                    post_data = {
                        'id': post_id,
                        'title': self.extract_smart_title(title + ' ' + description, group['type']),
                        'text': description,
                        'description': self.extract_smart_description(description),
                        'date': self.parse_rss_date(pub_date),
                        'url': link or f"{group['url']}/{post_id}",
                        'contact': self.extract_contact(description),
                        'photo_url': None,  # RSS –æ–±—ã—á–Ω–æ –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç –ø—Ä—è–º—ã–µ —Å—Å—ã–ª–∫–∏ –Ω–∞ —Ñ–æ—Ç–æ
                        'has_photo': 'photo' in description.lower() or '—Ñ–æ—Ç–æ' in description.lower(),
                        'type': group['type'],
                        'source': 'rss'
                    }
                    
                    if self.validate_post(post_data):
                        posts.append(post_data)
                        if len(posts) >= limit:
                            break
                
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ RSS —ç–ª–µ–º–µ–Ω—Ç–∞: {e}")
                    continue
            
            return posts
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ RSS: {e}")
            return []

    def parse_search_results(self, html: str, group: Dict, limit: int) -> List[Dict]:
        """–ü–∞—Ä—Å–∏–Ω–≥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ–∏—Å–∫–∞"""
        try:
            soup = BeautifulSoup(html, 'html.parser')
            posts = []
            
            # –ò—â–µ–º —Å—Å—ã–ª–∫–∏ –Ω–∞ t.me –≤ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞—Ö –ø–æ–∏—Å–∫–∞
            links = soup.find_all('a', href=re.compile(r't\.me/.+'))
            
            for link in links[:limit*2]:
                try:
                    href = link.get('href', '')
                    title_elem = link.find_parent().find('h3') or link
                    title = title_elem.get_text(strip=True) if title_elem else ''
                    
                    # –ü—ã—Ç–∞–µ–º—Å—è –Ω–∞–π—Ç–∏ –æ–ø–∏—Å–∞–Ω–∏–µ —Ä—è–¥–æ–º
                    desc_elem = link.find_parent().find_next('div') or link.find_parent()
                    description = desc_elem.get_text(strip=True)[:200] if desc_elem else title
                    
                    # –ò–∑–≤–ª–µ–∫–∞–µ–º ID –ø–æ—Å—Ç–∞
                    post_id = href.split('/')[-1] if '/' in href else f"search_{hash(href) % 10000}"
                    
                    post_data = {
                        'id': post_id,
                        'title': self.extract_smart_title(title, group['type']),
                        'text': description,
                        'description': self.extract_smart_description(description),
                        'date': self.generate_recent_date(),
                        'url': href if href.startswith('http') else f"https://t.me/{href}",
                        'contact': self.extract_contact(description),
                        'photo_url': None,
                        'has_photo': '—Ñ–æ—Ç–æ' in description.lower(),
                        'type': group['type'],
                        'source': 'search'
                    }
                    
                    if self.validate_post(post_data):
                        posts.append(post_data)
                        if len(posts) >= limit:
                            break
                
                except Exception as e:
                    continue
            
            return posts
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ –ø–æ–∏—Å–∫–∞: {e}")
            return []

    def parse_rss_date(self, date_str: str) -> str:
        """–ü–∞—Ä—Å–∏–Ω–≥ –¥–∞—Ç—ã –∏–∑ RSS"""
        try:
            # RFC 2822 —Ñ–æ—Ä–º–∞—Ç (—Å—Ç–∞–Ω–¥–∞—Ä—Ç RSS)
            from email.utils import parsedate_to_datetime
            dt = parsedate_to_datetime(date_str)
            return dt.strftime('%d.%m.%Y %H:%M')
        except:
            return "–ù–µ–¥–∞–≤–Ω–æ"

    def validate_post(self, post: Dict) -> bool:
        """–†–∞—Å—à–∏—Ä–µ–Ω–Ω–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è –ø–æ—Å—Ç–∞"""
        if not post or not isinstance(post, dict):
            return False
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ –ø–æ–ª—è
        required_fields = ['id', 'title', 'text', 'type']
        if not all(field in post for field in required_fields):
            return False
        
        text = post.get('text', '').lower()
        title = post.get('title', '').lower()
        combined_text = f"{title} {text}"
        
        # –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞
        if len(combined_text) < 20:
            return False
        
        # –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è –∂–∏–≤–æ—Ç–Ω—ã—Ö
        animal_keywords = {
            'cats': ['–∫–æ—Ç', '–∫–æ—à–∫', '–∫–æ—Ç–µ–Ω', '–º—É—Ä–∑', '–ø–∏—Ç–æ–º–µ—Ü', '–º—è—É'],
            'dogs': ['—Å–æ–±–∞–∫', '—â–µ–Ω', '–ø–µ—Å', '–ø–∏—Ç–æ–º–µ—Ü', '–ª–∞–π', '—Å–æ–±–∞—á']
        }
        
        # –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è –ø—Ä–∏—Å—Ç—Ä–æ–π—Å—Ç–≤–∞
        action_keywords = ['–∏—â–µ—Ç', '–¥–æ–º', '–ø—Ä–∏—Å—Ç—Ä–æ–π', '–æ—Ç–¥–∞', '–Ω–∞–π–¥–µ–Ω', '—Å–µ–º—å', '—Ö–æ–∑—è', '–ø–æ–º–æ']
        
        # –ò—Å–∫–ª—é—á–∞—é—â–∏–µ —Å–ª–æ–≤–∞
        exclude_keywords = ['–ø—Ä–æ–¥–∞–º', '–∫—É–ø–ª—é', '—É—Å–ª—É–≥', '—Ä–µ–∫–ª–∞–º', '—Å–ø–∞–º', '–ø–æ—Ä–Ω', '–∫–∞–∑–∏–Ω–æ']
        
        animal_type = post.get('type', 'cats')
        has_animal = any(keyword in combined_text for keyword in animal_keywords.get(animal_type, []))
        has_action = any(keyword in combined_text for keyword in action_keywords)
        has_exclude = any(keyword in combined_text for keyword in exclude_keywords)
        
        return has_animal and has_action and not has_exclude

    def get_fallback_posts(self, group_type: str, limit: int) -> List[Dict]:
        """–£–ª—É—á—à–µ–Ω–Ω—ã–µ —Ä–µ–∑–µ—Ä–≤–Ω—ã–µ –ø–æ—Å—Ç—ã"""
        # –°–Ω–∞—á–∞–ª–∞ –ø—ã—Ç–∞–µ–º—Å—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Å—Ç–∞—Ä—ã–π –∫—ç—à
        if self.backup_cache:
            cached = [p for p in self.backup_cache 
                     if group_type == 'all' or p['type'] == group_type]
            if cached:
                # –û–±–Ω–æ–≤–ª—è–µ–º –¥–∞—Ç—ã –≤ —Å—Ç–∞—Ä–æ–º –∫—ç—à–µ
                for post in cached:
                    post['date'] = self.generate_recent_date()
                    post['source'] = 'old_cache'
                return cached[:limit]
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã–µ –º–æ–∫–∏
        return self.generate_realistic_mocks(group_type, limit)

    def generate_realistic_mocks(self, group_type: str, limit: int) -> List[Dict]:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã—Ö –º–æ–∫–æ–≤"""
        if group_type == 'cats':
            templates = [
                {
                    'names': ['–ú—É—Ä–∫–∞', '–ë–∞—Ä—Å–∏–∫', '–°–Ω–µ–∂–æ–∫', '–†—ã–∂–∏–∫', '–¢–∏—à–∫–∞', '–ü—É—à–æ–∫', '–ú–∞—Ä—É—Å—è', '–í–∞—Å—å–∫–∞'],
                    'ages': ['2 –º–µ—Å—è—Ü–∞', '3-4 –º–µ—Å—è—Ü–∞', '6 –º–µ—Å—è—Ü–µ–≤', '8 –º–µ—Å—è—Ü–µ–≤', '1 –≥–æ–¥', '2 –≥–æ–¥–∞', '3 –≥–æ–¥–∞'],
                    'colors': ['—Ä—ã–∂–∏–π', '—Å–µ—Ä—ã–π', '—á–µ—Ä–Ω—ã–π', '–±–µ–ª—ã–π', '—Ç—Ä–µ—Ö—Ü–≤–µ—Ç–Ω–∞—è', '–ø–æ–ª–æ—Å–∞—Ç—ã–π', '—á–µ—Ä–Ω–æ-–±–µ–ª—ã–π'],
                    'traits': ['–∏–≥—Ä–∏–≤—ã–π', '–ª–∞—Å–∫–æ–≤—ã–π', '—Å–ø–æ–∫–æ–π–Ω—ã–π', '—É–º–Ω—ã–π', '–¥—Ä—É–∂–µ–ª—é–±–Ω—ã–π', '–Ω–µ–∑–∞–≤–∏—Å–∏–º—ã–π'],
                    'health': ['–ø—Ä–∏–≤–∏—Ç', '–∑–¥–æ—Ä–æ–≤', '–∫–∞—Å—Ç—Ä–∏—Ä–æ–≤–∞–Ω', '—Å—Ç–µ—Ä–∏–ª–∏–∑–æ–≤–∞–Ω–∞', '–æ–±—Ä–∞–±–æ—Ç–∞–Ω –æ—Ç –ø–∞—Ä–∞–∑–∏—Ç–æ–≤', '—á–∏–ø–∏—Ä–æ–≤–∞–Ω'],
                    'stories': [
                        '–Ω–∞–π–¥–µ–Ω –Ω–∞ —É–ª–∏—Ü–µ, –≤—ã—Ö–æ–∂–µ–Ω –≤–æ–ª–æ–Ω—Ç–µ—Ä–∞–º–∏',
                        '–æ—Å—Ç–∞–ª—Å—è –±–µ–∑ —Ö–æ–∑—è–µ–≤, –æ—á–µ–Ω—å —Å–∫—É—á–∞–µ—Ç',
                        '—Å–ø–∞—Å–µ–Ω –æ—Ç —Ö–æ–ª–æ–¥–æ–≤, –±–ª–∞–≥–æ–¥–∞—Ä–Ω—ã–π –∏ –¥–æ–±—Ä—ã–π',
                        '–ø–æ–¥–æ–±—Ä–∞–Ω –º–∞–ª—ã—à–æ–º, —Ç–µ–ø–µ—Ä—å –ø–æ–¥—Ä–æ—Å'
                    ]
                }
            ]
        else:
            templates = [
                {
                    'names': ['–ë–æ–±–∏–∫', '–®–∞—Ä–∏–∫', '–î—Ä—É–∂–æ–∫', '–õ–∞–π–∫–∞', '–î–∂–µ–∫', '–ë–µ–ª–∫–∞', '–†–µ–∫—Å', '–ù–∞–π–¥–∞'],
                    'ages': ['3 –º–µ—Å—è—Ü–∞', '4-5 –º–µ—Å—è—Ü–µ–≤', '6 –º–µ—Å—è—Ü–µ–≤', '8 –º–µ—Å—è—Ü–µ–≤', '1 –≥–æ–¥', '2 –≥–æ–¥–∞', '3 –≥–æ–¥–∞'],
                    'colors': ['—á–µ—Ä–Ω—ã–π', '–∫–æ—Ä–∏—á–Ω–µ–≤—ã–π', '–±–µ–ª—ã–π', '—Ä—ã–∂–∏–π', '–ø—è—Ç–Ω–∏—Å—Ç—ã–π', '—Å–µ—Ä—ã–π', '–∑–æ–ª–æ—Ç–∏—Å—Ç—ã–π'],
                    'traits': ['–∞–∫—Ç–∏–≤–Ω—ã–π', '–¥—Ä—É–∂–µ–ª—é–±–Ω—ã–π', '—É–º–Ω—ã–π', '–ø–æ—Å–ª—É—à–Ω—ã–π', '—ç–Ω–µ—Ä–≥–∏—á–Ω—ã–π', '—Å–ø–æ–∫–æ–π–Ω—ã–π'],
                    'health': ['–ø—Ä–∏–≤–∏—Ç', '–∑–¥–æ—Ä–æ–≤', '–∫–∞—Å—Ç—Ä–∏—Ä–æ–≤–∞–Ω', '—á–∏–ø–∏—Ä–æ–≤–∞–Ω', '–æ–±—Ä–∞–±–æ—Ç–∞–Ω –æ—Ç –ø–∞—Ä–∞–∑–∏—Ç–æ–≤', '–ø—Ä–æ–≥–ª–∏—Å—Ç–æ–≥–æ–Ω–µ–Ω'],
                    'stories': [
                        '–ø–æ—Ç–µ—Ä—è–ª—Å—è –∏ –Ω–µ —Å–º–æ–≥ –Ω–∞–π—Ç–∏ –¥–æ—Ä–æ–≥ –¥–æ–º–æ–π',
                        '—Ö–æ–∑—è–µ–≤–∞ –ø–µ—Ä–µ–µ—Ö–∞–ª–∏ –∏ –æ—Å—Ç–∞–≤–∏–ª–∏',
                        '—Ä–æ–¥–∏–ª—Å—è –Ω–∞ —É–ª–∏—Ü–µ, —Å–æ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω –≤–æ–ª–æ–Ω—Ç–µ—Ä–∞–º–∏',
                        '—Å–ø–∞—Å–µ–Ω –∏–∑ –ø—Ä–∏—é—Ç–∞, –∏—â–µ—Ç –ª—é–±—è—â—É—é —Å–µ–º—å—é'
                    ]
                }
            ]
        
        posts = []
        data = templates[0]
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã–µ –∏—Å—Ç–æ—Ä–∏–∏
        story_templates = [
            "{name} - {trait} {animal}, –≤–æ–∑—Ä–∞—Å—Ç {age}. {story}. {health}. {additional}",
            "{animal_emoji} {name} ({age}, {color}) —Å—Ä–æ—á–Ω–æ –∏—â–µ—Ç –¥–æ–º! {trait}, {health}. {story}.",
            "üè† –í –¥–æ–±—Ä—ã–µ —Ä—É–∫–∏ {name}, {color} {animal}. –í–æ–∑—Ä–∞—Å—Ç: {age}. {health}, {trait}. {story}. –û—Ç–¥–∞–µ—Ç—Å—è —Ç–æ–ª—å–∫–æ –≤ –Ω–∞–¥–µ–∂–Ω—ã–µ —Ä—É–∫–∏!",
            "‚ù§Ô∏è {name} –º–µ—á—Ç–∞–µ—Ç –æ —Å–µ–º—å–µ! {animal} {age}, {color} –æ–∫—Ä–∞—Å. {trait} –∏ {health}. {story}. –ü–æ–º–æ–∂–µ–º –Ω–∞–π—Ç–∏ —Å—á–∞—Å—Ç—å–µ!"
        ]
        
        for i in range(limit):
            name = random.choice(data['names'])
            age = random.choice(data['ages'])
            color = random.choice(data['colors'])
            trait = random.choice(data['traits'])
            health = random.choice(data['health'])
            story = random.choice(data['stories'])
            
            animal_emoji = 'üê±' if group_type == 'cats' else 'üê∂'
            animal_name = '–∫–æ—Ç' if group_type == 'cats' else '—Å–æ–±–∞–∫–∞'
            
            # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –¥–µ—Ç–∞–ª–∏
            additional_details = [
                '–ö –ª–æ—Ç–∫—É/–ø–æ–≤–æ–¥–∫—É –ø—Ä–∏—É—á–µ–Ω',
                '–° –¥–µ—Ç—å–º–∏ –ª–∞–¥–∏—Ç –æ—Ç–ª–∏—á–Ω–æ',
                '–° –¥—Ä—É–≥–∏–º–∏ –∂–∏–≤–æ—Ç–Ω—ã–º–∏ –¥—Ä—É–∂–∏—Ç',
                '–û—á–µ–Ω—å –±–ª–∞–≥–æ–¥–∞—Ä–Ω—ã–π –∏ –≤–µ—Ä–Ω—ã–π',
                '–ò–¥–µ–∞–ª—å–Ω–æ –ø–æ–¥–æ–π–¥–µ—Ç –¥–ª—è —Å–µ–º—å–∏',
                '–°—Ç–∞–Ω–µ—Ç –ø—Ä–µ–¥–∞–Ω–Ω—ã–º –¥—Ä—É–≥–æ–º'
            ]
            
            additional = random.choice(additional_details)
            
            # –í—ã–±–∏—Ä–∞–µ–º —Å–ª—É—á–∞–π–Ω—ã–π —à–∞–±–ª–æ–Ω
            template = random.choice(story_templates)
            
            description = template.format(
                name=name,
                age=age,
                color=color,
                trait=trait,
                health=health,
                story=story,
                additional=additional,
                animal_emoji=animal_emoji,
                animal=animal_name
            )
            
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫
            title_templates = [
                f'{animal_emoji} {name} –∏—â–µ—Ç –¥–æ–º',
                f'üè† {name} –≤ –¥–æ–±—Ä—ã–µ —Ä—É–∫–∏',
                f'‚ù§Ô∏è {name} –º–µ—á—Ç–∞–µ—Ç –æ —Å–µ–º—å–µ',
                f'üÜò {name} —Å—Ä–æ—á–Ω–æ –Ω—É–∂–µ–Ω –¥–æ–º',
                f'üíù {name} –∂–¥–µ—Ç —Å–≤–æ–∏—Ö –ª—é–¥–µ–π'
            ]
            
            posts.append({
                'id': f'mock_{group_type}_{i + 1000}',
                'title': random.choice(title_templates),
                'description': description,
                'text': description,
                'date': self.generate_recent_date(),
                'url': f'{self.groups[0]["url"] if group_type == "cats" else self.groups[1]["url"]}/{i + 1000}',
                'contact': self.generate_realistic_contact(),
                'photo_url': self.generate_animal_photo_url(group_type, i),
                'has_photo': True,
                'type': group_type,
                'source': 'smart_mock'
            })
        
        return posts

    def generate_animal_photo_url(self, animal_type: str, index: int) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å—Å—ã–ª–æ–∫ –Ω–∞ —Ñ–æ—Ç–æ –∂–∏–≤–æ—Ç–Ω—ã—Ö"""
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –±–µ—Å–ø–ª–∞—Ç–Ω—ã–µ —Å–µ—Ä–≤–∏—Å—ã —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏–π –∂–∏–≤–æ—Ç–Ω—ã—Ö
        if animal_type == 'cats':
            services = [
                f'https://cataas.com/cat?width=400&height=300&r={index}',
                f'https://placekitten.com/400/300?image={index % 16}',
                f'https://picsum.photos/400/300?random={index + 100}'  # Fallback
            ]
        else:
            services = [
                f'https://place.dog/400/300?id={index}',
                f'https://dog.ceo/api/breeds/image/random',  # API, –Ω—É–∂–Ω–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∞
                f'https://picsum.photos/400/300?random={index + 200}'  # Fallback
            ]
        
        return random.choice(services)

    def generate_recent_date(self) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –Ω–µ–¥–∞–≤–Ω–∏—Ö –¥–∞—Ç —Å —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã–º —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ–º"""
        # –ë–æ–ª—å—à–µ –ø–æ—Å—Ç–æ–≤ "—Å–µ–≥–æ–¥–Ω—è" –∏ "–≤—á–µ—Ä–∞"
        weights = [0.4, 0.3, 0.2, 0.1]  # 40% —Å–µ–≥–æ–¥–Ω—è, 30% –≤—á–µ—Ä–∞, –∏ —Ç.–¥.
        days_ago = random.choices([0, 1, 2, 3], weights=weights)[0]
        
        hours_ago = random.randint(0, 23)
        minutes_ago = random.randint(0, 59)
        
        recent_date = datetime.now() - timedelta(
            days=days_ago, 
            hours=hours_ago, 
            minutes=minutes_ago
        )
        
        return recent_date.strftime('%d.%m.%Y %H:%M')

    def generate_realistic_contact(self) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã—Ö –∫–æ–Ω—Ç–∞–∫—Ç–æ–≤"""
        # –†–µ–∞–ª—å–Ω—ã–µ –ø—Ä–µ—Ñ–∏–∫—Å—ã –º–æ–±–∏–ª—å–Ω—ã—Ö –æ–ø–µ—Ä–∞—Ç–æ—Ä–æ–≤ –†–æ—Å—Å–∏–∏
        prefixes = [
            '978', '977', '978',  # –ö—Ä—ã–º
            '903', '905', '906', '909',  # –ú–¢–°
            '910', '911', '912', '913', '914', '915', '916', '917', '918', '919',  # –ú–¢–°
            '920', '921', '922', '923', '924', '925', '926', '927', '928', '929',  # –ú–µ–≥–∞–§–æ–Ω
            '930', '931', '932', '933', '934', '936', '937', '938', '939',  # –ú–µ–≥–∞–§–æ–Ω
            '980', '981', '982', '983', '984', '985', '986', '987', '988', '989'   # –ë–∏–ª–∞–π–Ω
        ]
        
        # –†–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã–µ –∏–º–µ–Ω–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π
        usernames = [
            'volunteer_yalta', 'helper_animals', 'yalta_rescue', 'pet_help_yal',
            'animal_guardian', 'street_cats_yal', 'dog_rescue_yal', 'kind_hands',
            'pet_volunteer', 'animal_care_yal', 'furry_friends', 'paws_help'
        ]
        
        contacts = []
        
        # –¢–µ–ª–µ—Ñ–æ–Ω (80% –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å)
        if random.random() < 0.8:
            prefix = random.choice(prefixes)
            number = ''.join([str(random.randint(0, 9)) for _ in range(7)])
            phone = f"+7 {prefix} {number[:3]}-{number[3:5]}-{number[5:]}"
            contacts.append(phone)
        
        # Username (60% –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å)
        if random.random() < 0.6:
            username = random.choice(usernames)
            if random.random() < 0.3:  # –î–æ–±–∞–≤–ª—è–µ–º —Ü–∏—Ñ—Ä—ã
                username += str(random.randint(1, 99))
            contacts.append(f"@{username}")
        
        # WhatsApp (30% –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å, –µ—Å–ª–∏ –µ—Å—Ç—å —Ç–µ–ª–µ—Ñ–æ–Ω)
        if contacts and contacts[0].startswith('+7') and random.random() < 0.3:
            contacts.append("üì± WhatsApp")
        
        return ' ‚Ä¢ '.join(contacts) if contacts else "–ö–æ–Ω—Ç–∞–∫—Ç –≤ –≥—Ä—É–ø–ø–µ"

    # –û—Å—Ç–∞–ª—å–Ω—ã–µ –º–µ—Ç–æ–¥—ã –æ—Å—Ç–∞—é—Ç—Å—è –∏–∑ –∏—Å—Ö–æ–¥–Ω–æ–≥–æ –∫–æ–¥–∞
    def parse_html_content(self, html: str, group: Dict, limit: int) -> List[Dict]:
        """–£–ª—É—á—à–µ–Ω–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥ HTML —Å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–π –∑–∞—â–∏—Ç–æ–π"""
        try:
            soup = BeautifulSoup(html, 'html.parser')
            
            # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø—Ä–æ–≤–µ—Ä–∫–∏ –±–ª–æ–∫–∏—Ä–æ–≤–æ–∫
            blocking_indicators = [
                "cloudflare", "checking your browser", "ddos protection",
                "access denied", "403 forbidden", "rate limit",
                "too many requests", "blocked", "–∫–∞–ø—á–∞", "captcha"
            ]
            
            html_lower = html.lower()
            if any(indicator in html_lower for indicator in blocking_indicators):
                logger.warning("‚ö†Ô∏è –û–±–Ω–∞—Ä—É–∂–µ–Ω–∞ –±–ª–æ–∫–∏—Ä–æ–≤–∫–∞ –∏–ª–∏ –∑–∞—â–∏—Ç–∞")
                return []
            
            if len(html) < 2000:
                logger.warning("‚ö†Ô∏è HTML —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–π")
                return []
            
            # –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ —Å–µ–ª–µ–∫—Ç–æ—Ä—ã –¥–ª—è –ø–æ–∏—Å–∫–∞ —Å–æ–æ–±—â–µ–Ω–∏–π
            message_selectors = [
                'div.tgme_widget_message',
                'div[data-post]',
                'div.tgme_widget_message_wrap',
                '.tgme_widget_message',
                'article',
                '.message',
                '.post',
                '[class*="message"]',
                '[class*="post"]'
            ]
            
            messages = []
            for selector in message_selectors:
                found = soup.select(selector)
                if found and len(found) > 0:
                    messages = found
                    logger.info(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(found)} —ç–ª–µ–º–µ–Ω—Ç–æ–≤: {selector}")
                    break
            
            if not messages:
                # –ü—ã—Ç–∞–µ–º—Å—è –Ω–∞–π—Ç–∏ –ª—é–±—ã–µ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä—ã —Å —Ç–µ–∫—Å—Ç–æ–º
                messages = soup.find_all('div', string=re.compile(r'(–∫–æ—Ç|–∫–æ—à–∫|—Å–æ–±–∞–∫|—â–µ–Ω|–∏—â–µ—Ç|–¥–æ–º)', re.I))
                if messages:
                    logger.info(f"üìù –ù–∞–π–¥–µ–Ω–æ {len(messages)} —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –±–ª–æ–∫–æ–≤")
            
            if not messages:
                logger.warning("‚ùå –°–æ–æ–±—â–µ–Ω–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
                return []
            
            posts = []
            processed = 0
            
            for msg_elem in messages:
                if processed >= limit * 3:  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –æ–±—Ä–∞–±–æ—Ç–∫—É
                    break
                
                try:
                    post_data = self.parse_message_element(msg_elem, group)
                    if post_data and self.validate_post(post_data):
                        posts.append(post_data)
                        if len(posts) >= limit:
                            break
                
                except Exception as e:
                    logger.debug(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ —ç–ª–µ–º–µ–Ω—Ç–∞: {e}")
                    continue
                
                processed += 1
            
            logger.info(f"‚úÖ –ü–∞—Ä—Å–∏–Ω–≥ –∑–∞–≤–µ—Ä—à–µ–Ω: {len(posts)} –≤–∞–ª–∏–¥–Ω—ã—Ö –ø–æ—Å—Ç–æ–≤ –∏–∑ {processed} —ç–ª–µ–º–µ–Ω—Ç–æ–≤")
            return posts
            
        except Exception as e:
            logger.error(f"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ HTML: {e}")
            return []

    def parse_message_element(self, elem, group) -> Optional[Dict]:
        """–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥ —ç–ª–µ–º–µ–Ω—Ç–∞ —Å–æ–æ–±—â–µ–Ω–∏—è"""
        try:
            # ID –ø–æ—Å—Ç–∞
            post_id = (elem.get('data-post', '') or 
                      elem.get('data-message-id', '') or
                      elem.get('id', '') or
                      f"parsed_{hash(str(elem)[:200]) % 10000}")
            
            if '/' in str(post_id):
                post_id = str(post_id).split('/')[-1]
            
            # –¢–µ–∫—Å—Ç - –ø—Ä–æ–±—É–µ–º —Ä–∞–∑–Ω—ã–µ —Å–ø–æ—Å–æ–±—ã –∏–∑–≤–ª–µ—á–µ–Ω–∏—è
            text = self.extract_text_universal(elem)
            if not text or len(text) < 30:
                return None
            
            # –î–∞—Ç–∞ - —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ
            date_str = self.extract_date_universal(elem)
            
            # –§–æ—Ç–æ - —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ
            photo_url = self.extract_photo_universal(elem)
            
            return {
                'id': post_id,
                'text': text,
                'date': date_str,
                'url': f"{group['url']}/{post_id}",
                'title': self.extract_smart_title(text, group['type']),
                'description': self.extract_smart_description(text),
                'contact': self.extract_contact(text),
                'photo_url': photo_url,
                'has_photo': bool(photo_url),
                'type': group['type'],
                'source': 'parsed'
            }
            
        except Exception as e:
            logger.debug(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ —ç–ª–µ–º–µ–Ω—Ç–∞: {e}")
            return None

    def extract_text_universal(self, elem) -> str:
        """–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞"""
        # –ü—Ä–æ–±—É–µ–º —Ä–∞–∑–ª–∏—á–Ω—ã–µ —Å–µ–ª–µ–∫—Ç–æ—Ä—ã
        text_selectors = [
            '.tgme_widget_message_text',
            'div.tgme_widget_message_text', 
            '.message_text',
            '.text',
            '.content',
            '.post-content',
            'p',
            '.description'
        ]
        
        for selector in text_selectors:
            text_elem = elem.select_one(selector)
            if text_elem:
                text = text_elem.get_text(separator=' ', strip=True)
                if text and len(text) > 20:
                    return self.clean_text(text)
        
        # –ï—Å–ª–∏ —Å–µ–ª–µ–∫—Ç–æ—Ä—ã –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª–∏, –±–µ—Ä–µ–º –≤–µ—Å—å —Ç–µ–∫—Å—Ç
        full_text = elem.get_text(separator=' ', strip=True)
        return self.clean_text(full_text)

    def extract_date_universal(self, elem) -> str:
        """–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –¥–∞—Ç—ã"""
        date_selectors = [
            'time[datetime]',
            '.tgme_widget_message_date time',
            'time',
            '.date',
            '.time',
            '[datetime]',
            '.post-date'
        ]
        
        for selector in date_selectors:
            date_elem = elem.select_one(selector)
            if date_elem:
                # –ü—Ä–æ–±—É–µ–º –∞—Ç—Ä–∏–±—É—Ç datetime
                datetime_attr = date_elem.get('datetime')
                if datetime_attr:
                    try:
                        dt = datetime.fromisoformat(datetime_attr.replace('Z', '+00:00'))
                        return dt.strftime('%d.%m.%Y %H:%M')
                    except:
                        pass
                
                # –ü—Ä–æ–±—É–µ–º —Ç–µ–∫—Å—Ç —ç–ª–µ–º–µ–Ω—Ç–∞
                date_text = date_elem.get_text(strip=True)
                if date_text and len(date_text) > 3:
                    return date_text
        
        return self.generate_recent_date()

    def extract_photo_universal(self, elem) -> Optional[str]:
        """–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ñ–æ—Ç–æ"""
        photo_selectors = [
            '[style*="background-image"]',
            'img[src]',
            '[data-src]',
            '.photo img',
            '.image img',
            'picture img'
        ]
        
        for selector in photo_selectors:
            photo_elem = elem.select_one(selector)
            if photo_elem:
                # –ò–∑ style background-image
                style = photo_elem.get('style', '')
                if 'background-image' in style:
                    match = re.search(r"background-image:url\(['\"]?([^'\"]+)['\"]?\)", style)
                    if match:
                        return match.group(1)
                
                # –ò–∑ src –∏–ª–∏ data-src
                for attr in ['src', 'data-src', 'data-original']:
                    url = photo_elem.get(attr)
                    if url and ('http' in url or url.startswith('//')):
                        return url if url.startswith('http') else f"https:{url}"
        
        return None

    def clean_text(self, text: str) -> str:
        """–û—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞ –æ—Ç —Å–ª—É–∂–µ–±–Ω—ã—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤"""
        if not text:
            return ""
        
        # –£–¥–∞–ª—è–µ–º —Å–ª—É–∂–µ–±–Ω—ã–µ —Ñ—Ä–∞–∑—ã
        service_phrases = [
            r'Views\s*\d+',
            r'–ü—Ä–æ—Å–º–æ—Ç—Ä–æ–≤\s*\d+',
            r'Subscribe',
            r'–ü–æ–¥–ø–∏—Å–∞—Ç—å—Å—è',
            r'Forward',
            r'–ü–µ—Ä–µ—Å–ª–∞—Ç—å',
            r'Reply',
            r'–û—Ç–≤–µ—Ç–∏—Ç—å',
            r'\d+:\d+',  # –í—Ä–µ–º—è
            r'@\w+\s*‚Ä¢',  # –ê–≤—Ç–æ—Ä —Å —Ç–æ—á–∫–æ–π
        ]
        
        cleaned = text
        for pattern in service_phrases:
            cleaned = re.sub(pattern, '', cleaned, flags=re.IGNORECASE)
        
        # –£–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã
        cleaned = re.sub(r'\s+', ' ', cleaned).strip()
        
        return cleaned

    def extract_smart_title(self, text: str, animal_type: str) -> str:
        """–£–ª—É—á—à–µ–Ω–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –∑–∞–≥–æ–ª–æ–≤–∫–∞"""
        if not text:
            return self.get_default_title(animal_type)
        
        lines = [line.strip() for line in text.split('\n') if line.strip()]
        
        # –ò—â–µ–º —Å—Ç—Ä–æ–∫–∏ —Å –∫–ª—é—á–µ–≤—ã–º–∏ —Å–ª–æ–≤–∞–º–∏
        priority_keywords = ['–∏—â–µ—Ç –¥–æ–º', '–≤ –¥–æ–±—Ä—ã–µ —Ä—É–∫–∏', '–ø—Ä–∏—Å—Ç—Ä–æ–π—Å—Ç–≤–æ', '–Ω—É–∂–µ–Ω –¥–æ–º', '—Å—Ä–æ—á–Ω–æ']
        good_keywords = ['–∏—â–µ—Ç', '–¥–æ–º', '–ø—Ä–∏—Å—Ç—Ä–æ–π', '–æ—Ç–¥–∞', '–Ω–∞–π–¥–µ–Ω', '–ø–æ—Ç–µ—Ä—è–ª', '–ø–æ–º–æ']
        
        # –°–Ω–∞—á–∞–ª–∞ –∏—â–µ–º –≤—ã—Å–æ–∫–æ–ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω—ã–µ —Å—Ç—Ä–æ–∫–∏
        for line in lines[:5]:
            if len(line) > 10 and any(keyword in line.lower() for keyword in priority_keywords):
                title = self.format_title(line, animal_type)
                if title:
                    return title
        
        # –ü–æ—Ç–æ–º –æ–±—ã—á–Ω—ã–µ —Ö–æ—Ä–æ—à–∏–µ —Å—Ç—Ä–æ–∫–∏
        for line in lines[:5]:
            if len(line) > 15 and any(keyword in line.lower() for keyword in good_keywords):
                title = self.format_title(line, animal_type)
                if title:
                    return title
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–µ—Ä–≤—É—é —Å–æ–¥–µ—Ä–∂–∞—Ç–µ–ª—å–Ω—É—é —Å—Ç—Ä–æ–∫—É
        for line in lines[:3]:
            if len(line) > 20:
                title = self.format_title(line, animal_type)
                if title:
                    return title
        
        return self.get_default_title(animal_type)

    def format_title(self, text: str, animal_type: str) -> str:
        """–§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∑–∞–≥–æ–ª–æ–≤–∫–∞"""
        if not text:
            return ""
        
        # –û—á–∏—â–∞–µ–º –æ—Ç –º—É—Å–æ—Ä–∞
        title = re.sub(r'[^\w\s\-\.,!?–∞-—è—ë–ê-–Ø–Å]', ' ', text)
        title = re.sub(r'\s+', ' ', title).strip()
        
        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É
        if len(title) > 70:
            # –ü—ã—Ç–∞–µ–º—Å—è –æ–±—Ä–µ–∑–∞—Ç—å –ø–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—é
            sentences = re.split(r'[.!?]', title)
            if sentences and len(sentences[0]) <= 70:
                title = sentences[0] + ('.' if not sentences[0].endswith(('.', '!', '?')) else '')
            else:
                title = title[:67] + "..."
        
        # –î–æ–±–∞–≤–ª—è–µ–º —ç–º–æ–¥–∑–∏ –µ—Å–ª–∏ –Ω–µ—Ç
        emoji = 'üê±' if animal_type == 'cats' else 'üê∂'
        if not any(char in title for char in ['üê±', 'üê∂', '‚ù§Ô∏è', 'üè†', 'üíù']):
            title = f"{emoji} {title}"
        
        return title

    def get_default_title(self, animal_type: str) -> str:
        """–î–µ—Ñ–æ–ª—Ç–Ω—ã–µ –∑–∞–≥–æ–ª–æ–≤–∫–∏"""
        defaults = {
            'cats': [
                'üê± –ö–æ—à–∫–∞ –∏—â–µ—Ç –¥–æ–º',
                'üè† –ö–æ—Ç–µ–Ω–æ–∫ –≤ –¥–æ–±—Ä—ã–µ —Ä—É–∫–∏', 
                '‚ù§Ô∏è –ü—Ä–∏—Å—Ç—Ä–æ–π—Å—Ç–≤–æ –∫–æ—à–∫–∏',
                'üíù –ö–æ—à–µ—á–∫–∞ –º–µ—á—Ç–∞–µ—Ç –æ —Å–µ–º—å–µ'
            ],
            'dogs': [
                'üê∂ –°–æ–±–∞–∫–∞ –∏—â–µ—Ç –¥–æ–º',
                'üè† –©–µ–Ω–æ–∫ –≤ –¥–æ–±—Ä—ã–µ —Ä—É–∫–∏',
                '‚ù§Ô∏è –ü—Ä–∏—Å—Ç—Ä–æ–π—Å—Ç–≤–æ —Å–æ–±–∞–∫–∏', 
                'üíù –°–æ–±–∞—á–∫–∞ –º–µ—á—Ç–∞–µ—Ç –æ —Å–µ–º—å–µ'
            ]
        }
        
        return random.choice(defaults.get(animal_type, defaults['cats']))

    def extract_smart_description(self, text: str) -> str:
        """–£–ª—É—á—à–µ–Ω–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –æ–ø–∏—Å–∞–Ω–∏—è"""
        if not text:
            return ""
        
        # –£–¥–∞–ª—è–µ–º –∫–æ–Ω—Ç–∞–∫—Ç—ã –∏ —Å—Å—ã–ª–∫–∏ –¥–ª—è —á–∏—Å—Ç–æ–≥–æ –æ–ø–∏—Å–∞–Ω–∏—è
        clean_text = re.sub(r'(@\w+|https?://\S+|\+?[78][\d\s\-\(\)]{10,})', '', text)
        clean_text = re.sub(r'\s+', ' ', clean_text).strip()
        
        # –ï—Å–ª–∏ —Ç–µ–∫—Å—Ç –∫–æ—Ä–æ—Ç–∫–∏–π, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –∫–∞–∫ –µ—Å—Ç—å
        if len(clean_text) <= 200:
            return clean_text
        
        # –ü—ã—Ç–∞–µ–º—Å—è —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å —Ü–µ–ª—ã–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
        sentences = re.split(r'[.!?]+', clean_text)
        result = ""
        
        for sentence in sentences:
            sentence = sentence.strip()
            if not sentence:
                continue
                
            if len(result + sentence + '. ') <= 200:
                result += sentence + '. '
            else:
                break
        
        # –ï—Å–ª–∏ –ø–æ–ª—É—á–∏–ª—Å—è —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç, –±–µ—Ä–µ–º –±–æ–ª—å—à–µ
        if len(result) < 50 and len(clean_text) > 50:
            result = clean_text[:197] + "..."
        
        return result.strip() or clean_text[:200]

    def extract_contact(self, text: str) -> str:
        """–£–ª—É—á—à–µ–Ω–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–æ–Ω—Ç–∞–∫—Ç–æ–≤"""
        if not text:
            return "–°–º. –≤ –≥—Ä—É–ø–ø–µ"
        
        contacts = []
        
        # –†–æ—Å—Å–∏–π—Å–∫–∏–µ –Ω–æ–º–µ—Ä–∞ (–±–æ–ª–µ–µ —à–∏—Ä–æ–∫–∏–π –æ—Ö–≤–∞—Ç)
        phone_patterns = [
            r'\+?7[\s\-]?\(?9\d{2}\)?\s?[\d\s\-]{7,10}',
            r'\+?8[\s\-]?\(?9\d{2}\)?\s?[\d\s\-]{7,10}',
            r'9\d{2}[\s\-]?\d{3}[\s\-]?\d{2}[\s\-]?\d{2}',
            r'\+?7[\s\-]?\(?8\d{2}\)?\s?[\d\s\-]{7,10}',  # –ö—Ä—ã–º—Å–∫–∏–µ –Ω–æ–º–µ—Ä–∞
            r'8\s?\(?\d{3}\)?\s?[\d\s\-]{7,10}'
        ]
        
        for pattern in phone_patterns:
            phones = re.findall(pattern, text)
            if phones:
                phone = phones[0]
                # –û—á–∏—â–∞–µ–º –∏ —Ñ–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º –Ω–æ–º–µ—Ä
                clean_phone = re.sub(r'[\s\-\(\)]', '', phone)
                if len(clean_phone) >= 10:
                    if clean_phone.startswith('8'):
                        clean_phone = '+7' + clean_phone[1:]
                    elif clean_phone.startswith('9'):
                        clean_phone = '+7' + clean_phone
                    elif not clean_phone.startswith('+'):
                        clean_phone = '+7' + clean_phone[-10:]
                    
                    contacts.append(clean_phone)
                    break
        
        # Telegram username
        usernames = re.findall(r'@\w+', text)
        if usernames:
            contacts.append(usernames[0])
        
        # WhatsApp, Viber —É–ø–æ–º–∏–Ω–∞–Ω–∏—è
        messengers = re.findall(r'(WhatsApp|Viber|–≤–∞–π–±–µ—Ä|–≤–∞—Ç—Å–∞–ø|whatsapp|viber)', text, re.IGNORECASE)
        if messengers and contacts:
            contacts.append(f"üì± {messengers[0]}")
        
        return ' ‚Ä¢ '.join(contacts[:3]) if contacts else "–ö–æ–Ω—Ç–∞–∫—Ç –≤ –≥—Ä—É–ø–ø–µ"

    def get_cached_posts(self, group_type: str = 'all') -> List[Dict]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –ø–æ—Å—Ç–æ–≤ —Å —É–º–Ω—ã–º –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º"""
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç—å –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è
        should_update = (
            not self.last_update or 
            (datetime.now() - self.last_update).seconds > 1800 or  # 30 –º–∏–Ω—É—Ç
            len(self.posts_cache) == 0
        )
        
        # –ï—Å–ª–∏ –º–æ–∂–µ–º –ø–æ–ø—Ä–æ–±–æ–≤–∞—Ç—å –ø–∞—Ä—Å–∏–Ω–≥ - –¥–µ–ª–∞–µ–º —ç—Ç–æ
        if should_update and self.should_attempt_parsing():
            logger.info("üîÑ –ü–æ–ø—ã—Ç–∫–∞ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –ø–æ—Å—Ç–æ–≤...")
            try:
                fresh_posts = self.get_group_posts(group_type, 5)
                if fresh_posts:
                    # –§–∏–ª—å—Ç—Ä—É–µ–º –ø–æ —Ç–∏–ø—É –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
                    filtered_posts = [p for p in fresh_posts 
                                    if group_type == 'all' or p['type'] == group_type]
                    return filtered_posts
            except Exception as e:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è: {e}")
        
        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
        cached = [p for p in self.posts_cache 
                 if group_type == 'all' or p['type'] == group_type]
        
        if cached:
            return cached
        
        # –†–µ–∑–µ—Ä–≤–Ω—ã–π –∫—ç—à
        backup = [p for p in self.backup_cache 
                 if group_type == 'all' or p['type'] == group_type]
        
        if backup:
            # –û–±–Ω–æ–≤–ª—è–µ–º –¥–∞—Ç—ã –≤ —Ä–µ–∑–µ—Ä–≤–Ω–æ–º –∫—ç—à–µ
            for post in backup:
                post['date'] = self.generate_recent_date()
                post['source'] = 'backup_cache'
            return backup
        
        # –í –∫—Ä–∞–π–Ω–µ–º —Å–ª—É—á–∞–µ - –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º –º–æ–∫–∏
        return self.generate_realistic_mocks(group_type, 3)

# –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π –∫–ª–∞—Å—Å –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ –∏ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
class ParserMonitor:
    """–ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ —Ä–∞–±–æ—Ç—ã –ø–∞—Ä—Å–µ—Ä–∞"""
    
    def __init__(self, parser):
        self.parser = parser
        self.stats = {
            'total_attempts': 0,
            'successful_attempts': 0,
            'strategy_success': {},
            'last_success_time': None,
            'avg_response_time': 0,
            'error_types': {}
        }
    
    def log_attempt(self, strategy_name: str, success: bool, response_time: float = 0, error: str = None):
        """–õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–æ–ø—ã—Ç–∫–∏ –ø–∞—Ä—Å–∏–Ω–≥–∞"""
        self.stats['total_attempts'] += 1
        
        if success:
            self.stats['successful_attempts'] += 1
            self.stats['last_success_time'] = datetime.now()
            
            if strategy_name not in self.stats['strategy_success']:
                self.stats['strategy_success'][strategy_name] = 0
            self.stats['strategy_success'][strategy_name] += 1
            
            # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è –æ—Ç–≤–µ—Ç–∞
            current_avg = self.stats['avg_response_time']
            success_count = self.stats['successful_attempts']
            self.stats['avg_response_time'] = ((current_avg * (success_count - 1)) + response_time) / success_count
        
        if error:
            if error not in self.stats['error_types']:
                self.stats['error_types'][error] = 0
            self.stats['error_types'][error] += 1
    
    def get_health_status(self) -> Dict:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç—É—Å–∞ –∑–¥–æ—Ä–æ–≤—å—è –ø–∞—Ä—Å–µ—Ä–∞"""
        total_attempts = self.stats['total_attempts']
        success_rate = (self.stats['successful_attempts'] / total_attempts * 100) if total_attempts > 0 else 0
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Å—Ç–∞—Ç—É—Å
        if success_rate >= 80:
            status = "üü¢ –û—Ç–ª–∏—á–Ω–æ"
        elif success_rate >= 50:
            status = "üü° –£–¥–æ–≤–ª–µ—Ç–≤–æ—Ä–∏—Ç–µ–ª—å–Ω–æ"  
        elif success_rate >= 20:
            status = "üü† –ü–ª–æ—Ö–æ"
        else:
            status = "üî¥ –ö—Ä–∏—Ç–∏—á–Ω–æ"
        
        return {
            'status': status,
            'success_rate': round(success_rate, 1),
            'total_attempts': total_attempts,
            'successful_attempts': self.stats['successful_attempts'],
            'best_strategy': max(self.stats['strategy_success'].items(), key=lambda x: x[1])[0] if self.stats['strategy_success'] else None,
            'avg_response_time': round(self.stats['avg_response_time'], 2),
            'last_success': self.stats['last_success_time'].strftime('%d.%m.%Y %H:%M:%S') if self.stats['last_success_time'] else '–ù–∏–∫–æ–≥–¥–∞',
            'frequent_errors': sorted(self.stats['error_types'].items(), key=lambda x: x[1], reverse=True)[:3]
        }

# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è:
if __name__ == "__main__":
    parser = SuperRobustTelegramParser()
    monitor = ParserMonitor(parser)
    
    # –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ
    posts = parser.get_group_posts('cats', 3)
    print(f"–ü–æ–ª—É—á–µ–Ω–æ –ø–æ—Å—Ç–æ–≤: {len(posts)}")
    
    for post in posts:
        print(f"- {post['title']}")
        print(f"  –ò—Å—Ç–æ—á–Ω–∏–∫: {post['source']}")
        print(f"  –ö–æ–Ω—Ç–∞–∫—Ç: {post['contact']}")
        print()
